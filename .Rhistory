break
}
current_skip <- current_skip + take
if (sleep_time > 0) {
Sys.sleep(sleep_time)
}
}
}
if (length(results) == 0) {
out <- tibble::tibble()
} else {
out <- tibble::as_tibble(do.call(rbind, results))
}
attr(out, "meta") <- meta_list
out
}
#'   presolved = TRUE
#' )
#' }
#' @details
#' The function automatically splits requests into 2-day chunks because the API
#' does not allow long time intervals. Within each chunk, it paginates over all
#' results (in steps of `take`) until no more data are available. Rate limiting
#' is enforced to stay below the API limit of 100 requests per minute. Any
#' additional query parameters passed via `...` (e.g. `presolved`) are added to
#' each request.
JAOPuTo_get <- function(dataset,
endpoint,
start,
end,
skip = 0L,
take = 4000L,
rate_limit_per_minute = 30L,
...) {
base_url <- "https://publicationtool.jao.eu"
# Extra query-parameters via ...
extra_query <- list(...)
reserved <- c("fromUtc", "toUtc", "skip", "take")
# Negeer evt. conflicterende namen in ...
if (length(extra_query)) {
conflict <- intersect(names(extra_query), reserved)
if (length(conflict) > 0) {
warning(
"JAOPuTo_get(): ignoring extra query parameters that conflict with ",
paste(reserved, collapse = ", "), ": ",
paste(conflict, collapse = ", ")
)
extra_query <- extra_query[setdiff(names(extra_query), reserved)]
}
}
# Tijd tussen requests obv rate limiting
sleep_time <- if (is.finite(rate_limit_per_minute) &&
rate_limit_per_minute > 0) {
60 / rate_limit_per_minute
} else {
0
}
start_utc <- as.POSIXct(start, tz = "UTC")
end_utc   <- as.POSIXct(end, tz = "UTC") + lubridate::hours(1)
if (end_utc < start_utc) {
stop("JAOPuTo_get(): 'end' is before 'start'.", call. = FALSE)
}
to_utc_iso <- function(x) {
format(x, "%Y-%m-%dT%H:%M:%SZ", tz = "UTC")
}
# 2-daagse intervallen
days <- seq(as.Date(start_utc), as.Date(end_utc), by = "2 day")
dataset_clean  <- sub("/+$", "", dataset)
endpoint_clean <- sub("^/+", "", endpoint)
results   <- list()
meta_list <- list()
for (i in seq_along(days)) {
if (i == 1) {
day_start <- start_utc
} else {
day_start <- as.POSIXct(days[i], tz = "UTC")
}
if (i == length(days)) {
day_end <- end_utc
} else {
day_end <- as.POSIXct(days[i + 1], tz = "UTC") - 1
}
url <- paste(base_url, dataset_clean, endpoint_clean, sep = "/")
# Paginering binnen de 2-daagse chunk
current_skip <- skip
repeat {
base_query <- list(
fromUtc = to_utc_iso(day_start),
toUtc   = to_utc_iso(day_end),
skip    = current_skip,
take    = take
)
# Merge basis + extra query-params
query <- c(base_query, extra_query)
resp <- httr::GET(url, query = query)
if (httr::http_error(resp)) {
stop(
"JAOPuTo_get(): HTTP error (",
httr::status_code(resp), ") voor chunk ",
i, "/", length(days), " (skip = ", current_skip, ").",
call. = FALSE
)
}
txt    <- httr::content(resp, as = "text", encoding = "UTF-8")
parsed <- jsonlite::fromJSON(txt, simplifyDataFrame = TRUE)
if ("data" %in% names(parsed)) {
dat <- tibble::as_tibble(parsed$data)
meta_list[[length(meta_list) + 1L]] <-
parsed[setdiff(names(parsed), "data")]
} else {
dat <- tibble::as_tibble(parsed)
meta_list[[length(meta_list) + 1L]] <- NULL
}
if (nrow(dat) == 0) {
break
}
results[[length(results) + 1L]] <- dat
if (nrow(dat) < take) {
break
}
current_skip <- current_skip + take
if (sleep_time > 0) {
Sys.sleep(sleep_time)
}
}
}
if (length(results) == 0) {
out <- tibble::tibble()
} else {
out <- tibble::as_tibble(do.call(rbind, results))
}
attr(out, "meta") <- meta_list
out
}
source("R/JAOPuTo_biddingzones")
source("R/JAOPuTo_biddingzones.R")
source("R/JAOPuTo_get.R")
start = as.POSIXct("2024-01-01 00:00", "Europe/Brussels")
end = as.POSIXct("2024-12-31 23:00", "Europe/Brussels")
JAOPuTo_get(
dataset  = "core",
endpoint = "api/data/shadowPrices",
start    = start,
end      = end,
) |> # endpoint-specific data transformations
dplyr::mutate(dateTimeUtc = lubridate::ymd_hms(.data$dateTimeUtc, tz = "UTC"),
DateTime = lubridate::with_tz(.data$dateTimeUtc, "Europe/Brussels")) |>
dplyr::mutate(TSO = dplyr::case_match(.data$tso,
"Ceps" ~ "CEPS",
"Mavir" ~ "MAVIR",
"TennetBv" ~ "TenneT BV",
"Apg" ~ "APG",
"TransnetBw" ~ "TransnetBW",
"Pse" ~ "PSE",
"TennetGmbh" ~ "TenneT GmbH",
"Seps" ~ "SEPS",
"Hops" ~ "HOPS",
"Rte" ~ "RTE",
"Eles" ~ "ELES",
"NA" ~ NA,
.default = .data$tso)) |>
dplyr::left_join(readr::read_csv("inst/extdata/core_sgm_6th_release.csv") |>
dplyr::select(.data$CNE_EIC, "CNE_Lat" = .data$lat, "CNE_Lng" = .data$lng),
by = c("cnecEic" = "CNE_EIC")) |>
dplyr::select(.data$DateTime,
.data$TSO,
"CNE_Name" = .data$cnecName,
"CNE_EIC" = .data$cnecEic,
"Direction" = .data$direction,
"HubFrom" = .data$hubFrom,
"HubTo" = .data$hubTo,
.data$CNE_Lat,
.data$CNE_Lng,
"C_Name" = .data$contName,
"C_EIC" = .data$branchEic,
"ShadowPrice" = .data$shadowPrice,
"RAM" = .data$ram,
"RAM_MCP" = .data$ramMcp,
"minRAM_actual" = .data$minRamFactor,
"Imax" = .data$imax,
"Fmax" = .data$fmax,
"FRM" = .data$frm,
"Fref" = .data$fref,
"FCore" = .data$f0core,
"Fall" = .data$f0all,
"Fuaf" = .data$fuaf,
"AMR" = .data$amr,
"LTAmargin" = .data$ltaMargin,
"CVA" = .data$cva,
"IVA" = .data$iva,
"Ftotal_ltn" = .data$ftotalLtn,
"Maxz2zPTDF" = .data$maxZ2ZPtdf,
"ptdf_ALBE" = .data$hub_ALBE,
"ptdf_ALDE" = .data$hub_ALDE,
"ptdf_AT" = .data$hub_AT,
"ptdf_BE" = .data$hub_BE,
"ptdf_CZ" = .data$hub_CZ,
"ptdf_DE" = .data$hub_DE,
"ptdf_FR" = .data$hub_FR,
"ptdf_HR" = .data$hub_HR,
"ptdf_HU" = .data$hub_HU,
"ptdf_NL" = .data$hub_NL,
"ptdf_PL" = .data$hub_PL,
"ptdf_RO" = .data$hub_RO,
"ptdf_SI" = .data$hub_SI,
"ptdf_SK" = .data$hub_SK
)
#'   presolved = TRUE
#' )
#' }
#' @details
#' The function automatically splits requests into 2-day chunks because the API
#' does not allow long time intervals. Within each chunk, it paginates over all
#' results (in steps of `take`) until no more data are available. Rate limiting
#' is enforced to stay below the API limit of 100 requests per minute. Any
#' additional query parameters passed via `...` (e.g. `presolved`) are added to
#' each request.
JAOPuTo_get <- function(dataset,
endpoint,
start,
end,
skip = 0L,
take = 4000L,
rate_limit_per_minute = 80,
max_retries_429 = 5L,
...) {
base_url <- "https://publicationtool.jao.eu"
# Extra query-parameters via ...
extra_query <- list(...)
reserved <- c("fromUtc", "toUtc", "skip", "take")
# Negeer evt. conflicterende namen in ...
if (length(extra_query)) {
conflict <- intersect(names(extra_query), reserved)
if (length(conflict) > 0) {
warning(
"JAOPuTo_get(): ignoring extra query parameters that conflict with ",
paste(reserved, collapse = ", "), ": ",
paste(conflict, collapse = ", ")
)
extra_query <- extra_query[setdiff(names(extra_query), reserved)]
}
}
# Tijd tussen requests obv rate limiting (optioneel, extra voorzichtig)
sleep_time <- if (is.finite(rate_limit_per_minute) &&
rate_limit_per_minute > 0) {
60 / rate_limit_per_minute
} else {
0
}
start_utc <- as.POSIXct(start, tz = "UTC")
end_utc   <- as.POSIXct(end,   tz = "UTC") + lubridate::hours(1)
if (end_utc < start_utc) {
stop("JAOPuTo_get(): 'end' is before 'start'.", call. = FALSE)
}
to_utc_iso <- function(x) {
format(x, "%Y-%m-%dT%H:%M:%SZ", tz = "UTC")
}
# 2-daagse intervallen
days <- seq(as.Date(start_utc), as.Date(end_utc), by = "2 day")
dataset_clean  <- sub("/+$", "", dataset)
endpoint_clean <- sub("^/+", "", endpoint)
results   <- list()
meta_list <- list()
for (i in seq_along(days)) {
if (i == 1) {
day_start <- start_utc
} else {
day_start <- as.POSIXct(days[i], tz = "UTC")
}
if (i == length(days)) {
day_end <- end_utc
} else {
day_end <- as.POSIXct(days[i + 1], tz = "UTC") - 1
}
url <- paste(base_url, dataset_clean, endpoint_clean, sep = "/")
# Paginering binnen de 2-daagse chunk
current_skip <- skip
repeat {
base_query <- list(
fromUtc = to_utc_iso(day_start),
toUtc   = to_utc_iso(day_end),
skip    = current_skip,
take    = take
)
# Merge basis + extra query-params
query <- c(base_query, extra_query)
# ---- NIEUW: 429-handling met retries ----
attempt <- 1L
repeat {
resp <- httr::GET(url, query = query)
status <- httr::status_code(resp)
if (status == 429L) {
# Kijk of er een Retry-After header is
retry_after_hdr <- httr::headers(resp)[["Retry-After"]]
retry_after <- suppressWarnings(as.numeric(retry_after_hdr))
if (is.na(retry_after) || is.null(retry_after)) {
# fallback: exponentiÃ«le backoff (in seconden)
retry_after <- 30 * attempt  # 30s, 60s, 90s, ...
}
message(
"JAOPuTo_get(): HTTP 429 voor chunk ", i, "/", length(days),
" (skip = ", current_skip, "), poging ", attempt,
". Wachten ", retry_after, " seconden..."
)
if (attempt >= max_retries_429) {
stop(
"JAOPuTo_get(): te veel opeenvolgende 429-responses; ",
"aborting after ", max_retries_429, " retries.",
call. = FALSE
)
}
Sys.sleep(retry_after)
attempt <- attempt + 1L
next   # zelfde request opnieuw proberen
}
if (httr::http_error(resp)) {
stop(
"JAOPuTo_get(): HTTP error (",
status, ") voor chunk ",
i, "/", length(days), " (skip = ", current_skip, ").",
call. = FALSE
)
}
# Succes: uit deze inner repeat
break
}
# ---- Einde 429-handling ----
txt    <- httr::content(resp, as = "text", encoding = "UTF-8")
parsed <- jsonlite::fromJSON(txt, simplifyDataFrame = TRUE)
if ("data" %in% names(parsed)) {
dat <- tibble::as_tibble(parsed$data)
meta_list[[length(meta_list) + 1L]] <-
parsed[setdiff(names(parsed), "data")]
} else {
dat <- tibble::as_tibble(parsed)
meta_list[[length(meta_list) + 1L]] <- NULL
}
if (nrow(dat) == 0) {
break
}
results[[length(results) + 1L]] <- dat
if (nrow(dat) < take) {
break
}
current_skip <- current_skip + take
if (sleep_time > 0) {
Sys.sleep(sleep_time)
}
}
}
if (length(results) == 0) {
out <- tibble::tibble()
} else {
out <- tibble::as_tibble(do.call(rbind, results))
}
attr(out, "meta") <- meta_list
out
}
JAOPuTo_get(
dataset  = "core",
endpoint = "api/data/shadowPrices",
start    = start,
end      = end,
) |> # endpoint-specific data transformations
dplyr::mutate(dateTimeUtc = lubridate::ymd_hms(.data$dateTimeUtc, tz = "UTC"),
DateTime = lubridate::with_tz(.data$dateTimeUtc, "Europe/Brussels")) |>
dplyr::mutate(TSO = dplyr::case_match(.data$tso,
"Ceps" ~ "CEPS",
"Mavir" ~ "MAVIR",
"TennetBv" ~ "TenneT BV",
"Apg" ~ "APG",
"TransnetBw" ~ "TransnetBW",
"Pse" ~ "PSE",
"TennetGmbh" ~ "TenneT GmbH",
"Seps" ~ "SEPS",
"Hops" ~ "HOPS",
"Rte" ~ "RTE",
"Eles" ~ "ELES",
"NA" ~ NA,
.default = .data$tso)) |>
dplyr::left_join(readr::read_csv("inst/extdata/core_sgm_6th_release.csv") |>
dplyr::select(.data$CNE_EIC, "CNE_Lat" = .data$lat, "CNE_Lng" = .data$lng),
by = c("cnecEic" = "CNE_EIC")) |>
dplyr::select(.data$DateTime,
.data$TSO,
"CNE_Name" = .data$cnecName,
"CNE_EIC" = .data$cnecEic,
"Direction" = .data$direction,
"HubFrom" = .data$hubFrom,
"HubTo" = .data$hubTo,
.data$CNE_Lat,
.data$CNE_Lng,
"C_Name" = .data$contName,
"C_EIC" = .data$branchEic,
"ShadowPrice" = .data$shadowPrice,
"RAM" = .data$ram,
"RAM_MCP" = .data$ramMcp,
"minRAM_actual" = .data$minRamFactor,
"Imax" = .data$imax,
"Fmax" = .data$fmax,
"FRM" = .data$frm,
"Fref" = .data$fref,
"FCore" = .data$f0core,
"Fall" = .data$f0all,
"Fuaf" = .data$fuaf,
"AMR" = .data$amr,
"LTAmargin" = .data$ltaMargin,
"CVA" = .data$cva,
"IVA" = .data$iva,
"Ftotal_ltn" = .data$ftotalLtn,
"Maxz2zPTDF" = .data$maxZ2ZPtdf,
"ptdf_ALBE" = .data$hub_ALBE,
"ptdf_ALDE" = .data$hub_ALDE,
"ptdf_AT" = .data$hub_AT,
"ptdf_BE" = .data$hub_BE,
"ptdf_CZ" = .data$hub_CZ,
"ptdf_DE" = .data$hub_DE,
"ptdf_FR" = .data$hub_FR,
"ptdf_HR" = .data$hub_HR,
"ptdf_HU" = .data$hub_HU,
"ptdf_NL" = .data$hub_NL,
"ptdf_PL" = .data$hub_PL,
"ptdf_RO" = .data$hub_RO,
"ptdf_SI" = .data$hub_SI,
"ptdf_SK" = .data$hub_SK
)
sgm <- system.file("extdata", "core_sgm_6th_release.csv", package = "JAOPuTo")
sg
sgm
devtools::document()
devtools::check()
source("R/helpers.R")
getwd()
source("R/JAOPuTo_helpers.R")
read_core_sgm
read_core_sgm()
JAOPuTo_get(
dataset  = "core",
endpoint = "api/data/shadowPrices",
start    = start,
end      = end,
) |> # endpoint-specific data transformations
dplyr::mutate(dateTimeUtc = lubridate::ymd_hms(.data$dateTimeUtc, tz = "UTC"),
DateTime = lubridate::with_tz(.data$dateTimeUtc, "Europe/Brussels")) |>
dplyr::mutate(TSO = dplyr::case_match(.data$tso,
"Ceps" ~ "CEPS",
"Mavir" ~ "MAVIR",
"TennetBv" ~ "TenneT BV",
"Apg" ~ "APG",
"TransnetBw" ~ "TransnetBW",
"Pse" ~ "PSE",
"TennetGmbh" ~ "TenneT GmbH",
"Seps" ~ "SEPS",
"Hops" ~ "HOPS",
"Rte" ~ "RTE",
"Eles" ~ "ELES",
"NA" ~ NA,
.default = .data$tso)) |>
dplyr::left_join(readr::read_csv(read_core_sgm()) |>
dplyr::select(.data$CNE_EIC, "CNE_Lat" = .data$lat, "CNE_Lng" = .data$lng),
by = c("cnecEic" = "CNE_EIC")) |>
dplyr::select(.data$DateTime,
.data$TSO,
"CNE_Name" = .data$cnecName,
"CNE_EIC" = .data$cnecEic,
"Direction" = .data$direction,
"HubFrom" = .data$hubFrom,
"HubTo" = .data$hubTo,
.data$CNE_Lat,
.data$CNE_Lng,
"C_Name" = .data$contName,
"C_EIC" = .data$branchEic,
"ShadowPrice" = .data$shadowPrice,
"RAM" = .data$ram,
"RAM_MCP" = .data$ramMcp,
"minRAM_actual" = .data$minRamFactor,
"Imax" = .data$imax,
"Fmax" = .data$fmax,
"FRM" = .data$frm,
"Fref" = .data$fref,
"FCore" = .data$f0core,
"Fall" = .data$f0all,
"Fuaf" = .data$fuaf,
"AMR" = .data$amr,
"LTAmargin" = .data$ltaMargin,
"CVA" = .data$cva,
"IVA" = .data$iva,
"Ftotal_ltn" = .data$ftotalLtn,
"Maxz2zPTDF" = .data$maxZ2ZPtdf,
"ptdf_ALBE" = .data$hub_ALBE,
"ptdf_ALDE" = .data$hub_ALDE,
"ptdf_AT" = .data$hub_AT,
"ptdf_BE" = .data$hub_BE,
"ptdf_CZ" = .data$hub_CZ,
"ptdf_DE" = .data$hub_DE,
"ptdf_FR" = .data$hub_FR,
"ptdf_HR" = .data$hub_HR,
"ptdf_HU" = .data$hub_HU,
"ptdf_NL" = .data$hub_NL,
"ptdf_PL" = .data$hub_PL,
"ptdf_RO" = .data$hub_RO,
"ptdf_SI" = .data$hub_SI,
"ptdf_SK" = .data$hub_SK
)
devtools::document()
devtools::check()
devtools::document()
devtools::check()
devtools::document()
devtools::check()
devtools::document()
devtools::check()
